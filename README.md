# BackMarket Technical Assignement

The goal of this assignement is to create a datapipeline to filter valid and invalid records for the csv file :
 https://backmarket-data-jobs.s3-eu-west-1.amazonaws.com/data/product_catalog.csv

This must be done by implementing a job in Python that will do the following steps :
1. Download and read the file: product_catalog.csv locally
2. Transform the file from CSV to Parquet format locally
3. Separate the valid rows from the invalid ones into two separate files: the business wants only the product with an image but wants to archive the invalids rows

To meet these requirements, I choose to use the Spark framework using pyspark and I emulate a Datalake locally (by running ./scripts/generate_datalake.py


## Requirements

name  | version
------------- | -------------
Spark  | >= 2.4
Python  | >= 3.4
Java jdk | 8
os | linux


For other requirements for python code, simply run :

`$ pip3 install -r requirements.txt`

This command will install wget and pyspark packages for python3.


## Repository Architecture

This repository provided all you need to run the pyspark job locally

### Datalake script

The datalake folder will be generated by the script : ./scripts/generate_datalake.py
This script will generate folder that represent a datalake architecture locally and upload the input file directly in the ingress folder.
image
The datalake folder it's divided in 3 differents zones :
    - ingress : contain the input file product_catalog.csv
    - raw :
      - copyRawFiles : contain the input file in parquet format product_catalog.parquet
      - valid : contain data from the input file that are concidered as valid (column image not null) in csv format
      - invalid : contain data from the input file that are concidered as invalid (column image null) in csv format
    - archive : contain the input file csv (will be moved from ingress folder at the end of the data pipeline)
The can be used several times to rebuild all the datalake architecture.

### Pyspark Job
