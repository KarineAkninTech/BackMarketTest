# BackMarket Technical Assignement

The goal of this assignement is to create a data pipeline to filter valid and invalid records for the csv file :
 https://backmarket-data-jobs.s3-eu-west-1.amazonaws.com/data/product_catalog.csv

This must be done by implementing a job in Python that will do the following steps :
1. Download and read the file: product_catalog.csv locally
2. Transform the file from CSV to Parquet format locally
3. Separate the valid rows from the invalid ones into two separate files: the business wants only the product with an image but wants to archive the invalids rows

To meet these requirements, I choose to use the Spark framework using pyspark and I also decided to emulate a Datalake locally (by running ./scripts/generate_datalake.py)


## Requirements

name  | version
------------- | -------------
Spark  | >= 2.4
Python  | >= 3.4
Java jdk | 8
os | linux


For other requirements for python code, simply run :

`$ pip3 install -r requirements.txt`

This command will install wget and pyspark packages for python3.


## Repository Architecture

This repository provided all you need to run the pyspark job locally.

### Datalake script

The datalake folder will be generated by the script :
`$ python3 ./scripts/generate_datalake.py`

This script will generate folder that represent a datalake architecture locally and upload the input file directly in the ingress folder.

![datalake architecture](https://github.com/KarineAkninTech/BackMarketTest/blob/dev/images/datalake-architecture.jpg)

The datalake folder it's divided in 3 differents zones :
- ingress : contain the input file product_catalog.csv
- raw :
  - copyRawFiles : folder that will contain the input file in parquet format product_catalog.parquet
  - valid : folder that will contain data from the input file that are concidered as valid (column image not null) in csv format
  - invalid : folder that will contain data from the input file that are concidered as invalid (column image null) in csv format
- archive : folder that will contain the input file csv (will be moved from ingress folder at the end of the data pipeline)

The script generate_datalake.py can be used several times to rebuild all the datalake architecture.


### Pyspark Job
The pyspark job can be found in the folder : transformater/transform.py

It can be run by using the submit.sh script on the root directory. The script will automatically generate valid absolute paths to interact
with the datalake folder, those would be given as arguments to the spark-submit command.

#### Data Pipeline
This simple job will implement all the data pipeline in several steps :

![dag](https://github.com/KarineAkninTech/BackMarketTest/blob/dev/images/DAG.jpg)

- step 1 : create the SparkSession Object to use Spark Dataframe API
- step 2 : read the csv file stored in datalake/ingress, will generate a Spark Dataframe containing data from csv with infering schema from header
- step 3 : write the Dataframe to parquet file in datalake/raw/copyRawFiles/product_catalog/, with coalesce(1) to generate a unique parquet file (just one PART)
- step 4 : read the parquet file and cache the generate Dataframe in memory level, infering the schema : for next transformation, because of laziness, it is recommanded to cache the Dataframe to not recompute it twice
- step 5 : two filters will be performed on the cached Dataframe. One for valid data on col('image') != Null and another for invalid data on col('image') == Null. Will generate two Dataframes
- step 6 : write the two Dataframes as csv on the folder datalake/raw/valid/product_catalog/ and datalake/raw/invalid/product_catalog/, keeping the header and use of coalesce(1) to generate a unique csv per write action
- step 7 : move the csv file from datalake/ingress/ to datalake/archive/product_catalog/, to prevent for recomputing twice the same file


#### Errors handling

To prevent for duplicating data, a specific strategy has been done :
- case 1 : the script ends properly :
  - the file will be moved from ingress to archive at the end of the spark job, meaning all previous actions have been done
  - a simple check of the file existing on the archive folder may prevents from recompute it twice
- case 2 : the script did not end properly :
  - the file remains in the ingress folder
  - all the write action will overwrite the actual contents, meaning even if one write has been performed and not the others, every output files from every steps would be overwriten (no duplicate data)


## Running the pyspark job

Be sure to meet all the requirements in the requirements section above

### Step 1
install all requirements for python3 :

`$ pip3 install -r requirements.txt`


### Step 2
launch your spark cluster in standalone mode :

`$ ./sbin/start-master.sh`

### Step 3
Generate the datalake infrastructure :

`$ python3 ./scripts/generate_datalake.py`


### Step 4
Submit pyspark job to Spark master locally :

`$ chmod +x submit.sh`

`$ ./submit.sh`

### Step 5
During the pyspark job executing, a log file will be created in the root folder :

`$ cat transform.log`

`$ tail -500f transform.log` to read it during execution


## Code Improvement

### Running on a real Spark Cluster

Spark has been designed to be used on a cluster of servers. For bigger files, I would preconised to get all the power of Spark on a AWS Cluster :
- you can setup an AWS EMR cluster with at least 1 master and 2 slaves nodes : this will speedup the computation by breaking down the cached dataframe on several nodes those would compute the filter transformations in parallel on smaller chunk of the dataframe
- storing data locally is not a good idea : better to use an AWS S3 Bucket with the same architecture than the datalake folder. The transform.py script must be readapted to interact with an S3 Bucket : the given paths to the spark-submit can be changed without rewritting the code but all the functions that work with linux system (like os.mkdir()) must be readapted for a S3 Bucket environment. Be sure that the IAM roles and policies of the EMR Cluster are compliant to work with your S3 Bucket.
- All coalesce(1) must be getting out of the code because they will break down performance on big dataset : to have a unique parquet or csv file, will forced spark to have a unique partition on a single node. A better solution would be to have each slave nodes writing their own parts.
- To run the script on a batch of data, you can use pattern to get all the files product_catalog_timestamp.csv and then make a loop to compute the job for all the files that match the given pattern.
- For error handling I would suggest to use an orchestrator like Airflow or Oozie. Also, log files must be written out of the Spark cluster, in a specific zone of the S3 Bucket for example.


### PartitionBy Approach

The actual approach of filtering twice the same cached dataframe to generate valid and invalid data envolves computing twice the same dataframe. Another approach can be done :
- generating an extra column to the dataframe : withcolumn('flag') and when(col('image') != Null, 'valid').otherwise('invalid'). This extra column 'flag' will contain 'valid' or 'invalid' depending of col('image') values.
- then write the dataframe to csv using partitionBy('flag') method, will automatically generate two partitions : one for valid and another for invalid data
- you must combine partitionBy() with repartition() to be sure that multiple Parts would be generated per partitions : it is not a good idea to have only one Part per partition on big dataset.
- This technic avoids to read the dataframe twice (by filtering twice) but the repartition() method may cause a lot of shuffle. Sometimes memory actions are much more speed that actions involving network. The two solutions must be benchmarked to take a good decision for production level
- Also, partitioning on disk is generaly used to speedup futur queries : here, one of the partition may be never used, that's why I choose the filtering twice approach.