# BackMarket Technical Assignement

The goal of this assignement is to create a datapipeline to filter valid and invalid records for the csv file :
 https://backmarket-data-jobs.s3-eu-west-1.amazonaws.com/data/product_catalog.csv

This must be done by implementing a job in Python that will do the following steps :
1. Download and read the file: product_catalog.csv locally
2. Transform the file from CSV to Parquet format locally
3. Separate the valid rows from the invalid ones into two separate files: the business wants only the product with an image but wants to archive the invalids rows

To meet these requirements, I choose to use the Spark framework using pyspark and I emulate a Datalake locally (by running ./scripts/generate_datalake.py


## Requirements

name  | version
------------- | -------------
Spark  | >= 2.4
Python  | >= 3.4
Java jdk | 8
os | linux


For other requirements for python code, simply run :

`$ pip3 install -r requirements.txt`

This command will install wget and pyspark packages for python3.


## Repository Architecture

This repository provided all you need to run the pyspark job locally

### Datalake script

The datalake folder will be generated by the script : ./scripts/generate_datalake.py

This script will generate folder that represent a datalake architecture locally and upload the input file directly in the ingress folder.

![]
(https://github.com/KarineAkninTech/BackMarketTest/blob/dev/images/datalake-architecture.jpg)

The datalake folder it's divided in 3 differents zones :
- ingress : contain the input file product_catalog.csv
- raw :
  - copyRawFiles : folder that will contain the input file in parquet format product_catalog.parquet
  - valid : folder that will contain data from the input file that are concidered as valid (column image not null) in csv format
  - invalid : folder that will contain data from the input file that are concidered as invalid (column image null) in csv format
- archive : folder that will contain the input file csv (will be moved from ingress folder at the end of the data pipeline)

The can be used several times to rebuild all the datalake architecture.

### Pyspark Job
mettre step du datalake Ã  chaque read, write